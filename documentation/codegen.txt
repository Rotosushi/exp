// Copyright (C) 2024 Cade Weinberg
// 
// This file is part of exp.
// 
// exp is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
// 
// exp is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
// 
// You should have received a copy of the GNU General Public License
// along with exp.  If not, see <http://www.gnu.org/licenses/>.

some basic questions:
1 - what is the in memory representation (IMR) of 
    the intermediate representation (IR)?
2 - how does the parser create the IR from the source code?

3 - what is the IMR of the control flow graph (CFG)?
4 - how does the CFG get created to represent the source code?

5 - how can the IR be converted into SSA form?

6 - how do we allocate global constants and new types when BC IR
    is optimized towards working with local variables?

7 - how does the backend work with the register allocator
    to generate target machine assembly?
7a - how does the register allocator use the IR to allocate variables?

considerations of these questions:

  1 - "What is the IMR of the IR?"
      There are two representations of IR that I know of. 
      one is the Abstract Syntax Tree (AST)
      and the other is Bytecode (BC)
      
      we can use a Tree based approach, even if we created a variant of 
      the AST that wasted less space, and that would be fairly standard,
      as far as I can tell.
      However, there are some real advantages to bytecode
      A - time and space efficiency
        reading and writing BC is as efficient as it gets when dealing 
        with large amounts of data. as it can boil down to a memcpy in the best 
        possible case, a memmove in overlapping cases. and we can index into the 
        array of BC anywhere and it is as fast as indexing into an array.
        Now, with a variable length encoding of the BC, we cannot simply 
        index into the bytecode array at any index and be sure that we are landing
        on a correct instruction. However if we have a static length encoding, 
        we can simply index into the array anywhere and start reading and writing
        instructions. This is in contrast to the classical IR 
        Abstract Syntax Trees (AST). Which given their tree like structure means 
        that we must process the entire tree to reach leaf nodes, which can 
        slow down modifying the structure of the IR itself during optimization.
        additionally, given that AST's are a node based data structure, they suffer 
        from "pointer-chasing" or "data-fragmentation", where sections of the AST 
        have the potential to be allocated far away from eachother. (farther than 
        a cache line, or cache page), and as such there are more cache misses when 
        processing the entire AST, or any part of it. when you access BC however, 
        you are guarateed to cache the next few instructions. meaning processing 
        the whole BC at once is more efficient in time. additionally, node based 
        data structures must store some amount of pointers per unit of data. 
        in this case each AST node. meaning that there is some amount of storage 
        overhead per node in the AST. whereas BC is a dynamically allocated array,
        and so there is storage overhead per array of BC. meaning there is 
        generally speaking less overhead in the IMR of the source code. There are 
        cases where node based operations happen in a small amount of constant time,
        and the same operations on a dynamic array take linear time. however, given
        general use (a mix of constant/linear operations on both structures) 
        dynamic arrays are generally faster overall than node based data structures.
        (see: https://www.youtube.com/watch?v=YQs6IC-vgmo&t=0s )
        (additional note: there is often additional overhead per malloc call, because each 
        chunk of allocated memory needs a control block or metadata block which is 
        used by the allocation strategy. (I'm not well versed in malloc implementations,
        I just know about some of the simpler ones, this note could be more or less 
        true than I made it out to be.))


      There are some challenges with BC as opposed the the AST from a 
      logical perpective. (skill issue tbh) where logically the AST 
      more completely represents the input source code. so I can easily reason 
      about optimization and CFG generation and so on and so forth. whereas there 
      is a translation step in my mind when thinking about applying these concepts 
      to BC. since I consider this a skill issue, I'm not counting it against BC 
      as a real negative. as once I understand how to implement these things with 
      respect to BC the challenge goes away.

      If we consider (abstactly) metadata that is used during compilation that is 
      related to specific expressions or sub-expressions, 
      with the AST style, we can easily associate 
      that metadata with any or all or none of any individual node in the AST, 
      we can simply add such metadata to the AST node itself. This is logically 
      simple and easy to implement, it is also wasteful of space if those bits of 
      metadata are not useful for each node in the AST that the AST is composed of,
      and given that the AST represents syntax, and not semantics, there is usually 
      nodes in the AST which are just used to link together the actually useful 
      peices of information. anytime this happens we are allocating space for metadata 
      that isn't used, which is wasted space.
      if we instead allocate metadata in a separate structure, and provide a link which 
      associates the metadata with the particular expression or piece of expression, we 
      can both, allocate only as much metadata as we need, and associate that 
      metadata with instructions held within BC as easily as with AST nodes. 
      with no overhead to the nodes or the BC instructions.

  2 - "how does the parser create the IR from the source code?"
      The parser can generally create bytecode the same way it creates 
      any IR, it simply uses the information it parses to decide what 
      to create and what to associate it with. However there are some 
      practical questions that need to be answered. such as how does 
      the parser create bytecode instructions which reference local 
      variables, when the bytecode instructions themselves cannot encode 
      a pointer to the variable name, or anything that simple. (as would 
      be done with a tree like structure, where the parser would create 
      nodes that would simply hold the variable name directly.)

      what I am proposing is to take a page from both clox local variables, and 
      LLVM local variables. (clox takes a page from lua for locals, so it's
      transitively taking a page from lua. but I digress.)
      the numbered locals approach of LLVM, local names can 
      be easily generated in a new local scope by taking the names %0, %1, %2, ...
      and how locals are stored on the stack in clox, that is, the value of the 
      local is stored in some stack slot, and instructions which reference that 
      local simply encode an offset from a "frame pointer" in order to access the 
      value of the local. 

      so when we generate an instruction which uses a local variable (%n),
      we encode an index (n) which when added to a frame pointer (f) computes a pointer to the 
      value of the local. (locals + n + f -> Value *) 
      (and this approach works for parameters/arguments as well, they can simply take 
      up the first slots within a functions frame.)
      and when we want to allocate a new local we can simply use the next available 
      slot (%n+1)
      and in order to mimic SSA form, we generate new locals as the result of each 
      bytecode instruction, (add %n+1, %n, %n-1) %n+1 is a new slot, %n and %n-1 are 
      both existing locals.

      the parser should be able to easily generate instructions which create these 
      indices, say [store %n, 12]
      but how will the parser know when to use a previous variable?
      like in [add %n+2, %n+1, %n]. that is, this is where we would normally 
      rely on the existence of a mapping of name to value, the parser just needs 
      to emit bytecode which pushes the name on the stack, and because the VM 
      is stack based, the other instructions implicitly use the values from 
      the top of the stack as their arguments. this is why the add instruction 
      currently has zero operands. all of its operands are currently stack slots.

      I want to say, lets create a map from variable-name to index, so when the 
      user types "x" the parser can use that to look up which index the local "x"
      is stored in. But how does that work when we assign x multiple times, 
      and "x" becomes stored in multiple locations? we could update the mapped index of 
      "x" to refer to the most recent SSA version of "x", I think that might work.
      when the parser sees something like "const x = 5;" we can emit an instruction like
      "load %n, 5" and record in the "map" ("x", n). when the parser later encounters 
      "x += 1;" we can emit an instruction like "add %n+k, %n, 1" and update the map 
      ("x", n+k).
      
      well, lets say that works for the parser, and now we can generate BC from source 
      code directly. How does a later analysis "know" that "x" was at (n, n+k, ...)
      if the index is overwritten? say we need to emit an error during analysis 
      that wants to use the name of the variable. or, we want to optimize some usage 
      of "x", how can we "know" what the chain of "x"s is?


  3 - "What is the IMR of the Control Flow Graph?"
      I know of two ways to store graphs in memory. 
      as a vector of linked lists (adjacency list)
      or a dynamic array treated as a matrix (adjacency matrix)
      now, normally I am all for not using linked lists, 
      when a dynamic array is available (for all the usual reasons). 
      However this is one situation where there might be a good argument.
      given that most vertecies in the graph are going to have < 10 outgoing 
      edges. That is, most functions are going to call out to < 10 other functions.
      so this means that even though a given program is going to have a lot of 
      vertecies, (one per function), each vertex is going to have very few edges.
      (approx < 10). This means that most of the storage allocated for an adjacency
      matrix will be wasted space. the hueristic is that since an adjacency matrix 
      will always use n^2 space (where n is the number of vertecies), if the number 
      of edges is significantly smaller than n^2, then most of the graphs storage 
      will remain unused. (the actual calculation should take into account that 
      the list approach has storage overhead per edge, so for some number of 
      vertecies (k) even if your graph is sparse a matrix will be less memory overall
      than an adjacency list. my bet is that k is on the order of 100.
      However given that this is a compiler, and real world programs routinely have
      hundreds if not thousands of functions, I's say the hueristic holds for the 
      current use-case.)
      so an adjacency list is going to be the choice.

  4 - "How does the CFG get created to represent the source code?"
      It would be nice if the parser could build the control flow graph as 
      it built the IR.

  5 - "how can the IR be converted into SSA form?"
      maybe IR can be generated in SSA form or close to it? 
      SSA essentially means each variable is assigned only once. 
      When I recall what LLVM does with it's IR there is a set 
      of local variables %0, %1, %2, etc..., which are generated within the IR. 
      when an instruction produces a resulting value, that value is placed into a new 
      local. according to LLVM docs, this not only simplifies the code generated, 
      it makes it easier to translate into SSA form, because each symbol is already only 
      assigned once. each function gets to start it's local's by starting the count over.
      if we mimic that behavior in the IR we generate, by maintaining a growing set of 
      virtual registers, does this get us closer to a true SSA form? 
      a classic opcode like [add <r0>, <r1>] would perform addition and then 
      assignment of r0, but llvm would state that like [%2 = add <type> %0, %1]
      where %2 is never assigned again after creation. This is SSA form, except for 
      phi nodes. which only occur when branching is introduced.

      in this scenario %0, %1, %2, ..., are all virtual register indexes, in our IR.
      and we can simply have a 3 argument opcode, [add, %2, %0, %1], which keeps SSA,
      and keeps a minimal encoding. the only restriction is the size of our indices 
      into the register set.

      one thing to note is that even considering interpretation, a functions call frame could 
      include an offset into the actual array of virtual registers (a register frame pointer if 
      you would allow the analogy) allowing each function call to use low numbered register 
      indexes in the instruction encodings and yet still index into a "infinite" set of 
      virtual registers. 

      a few things come to mind:
      A - we have to decide on a static length for our encoding. 64 is natural, 32 could 
      be slightly more efficient from a data perspective (64 bit word size means more instructions 
      will be loaded into cache when reading BC, allowing faster processing of BC),
      and portable to 32 bit machines. 64 bit encoding seems very difficult to encode into a 32 
      bit architecture. whereas the other way around "just works."

      B - we have to separate functions and values.
      this is going to simplify the treatment of values considerably, though we now have 
      to build up some extra data-structures to manage functions. 
      (they can't be stored in the constants array, in fact I think this means we switch 
      to a register based VM instead of a stack based VM.)
      and on this point, more fundamentally, we need to separate defintion of symbols from 
      the instructions entirely. That is the parser needs to build symbols, and insert them
      into the global symbol table. AND there is no more global bytecode, there is only 
      local bytecode. AND local bytecode needs to be split up into a set of basic blocks.
      though, to be completely fair, this only needs to happen to support local control flow.
      interprocedural control flow can very well occur with each function only having a 
      single basic block.

      C - do we avoid register allocation entirely with this approach?
      or do we put the cart before the horse with this?
      such as do we need to worry about "spilling" registers to the stack?

      D - how do we handle Algebraic Data Types (ADT)? that is Structs and Unions?
      do these always live on the stack?
      and can we handle functions as values via closures written on top of an ADT?


      2 - if BC is the IR, then we already generate variable length BC.
      I think it would be advantageous to switch to a static length BC.
      because static length instructions make it possible to start indexing 
      into an array of BC without first scanning the entire BC up to that point.
      This simplifies the implementation of replacing/removing/injecting instructions 
      in BC.
      it is mostly identical to have the parser generate any form of IR, we just need 
      to set up the correct functions for the parser to call. The main difference could 
      be that since the parser has to define symbols, this means that the parser has to 
      pass around more data than it does currently. currently it is a side effect that 
      the top of the stack gets populated with the result of an affix expression, such 
      that the parser of constant definitions only needs to emit the opcode for "define 
      a constant" after it parses the affix expression, the bytecode is then in the 
      correct order for interpretation. however, if the parser needs to generate the 
      symbol defintion, then it has to have either the value of the symbol or something 
      that can be used to synthesize the value of the symbol. this something must be 
      returned from the parser of affix expressions to the parser of constant definitions
      such that a global symbol can be generated. 

      given that it is an affix expression we are talking about, the same thing that a 
      functions body is composed of, it makes sense to use a basic block here. It can 
      represent something which can be used to synthesize the value of the symbol.

      an additional question is "How can we represent the phi function?"
      which is used to coalese values after branches. The difficuly is 
      that the phi function can have arbitrary fanin, that is any number 
      of incoming values which -could- have initialized the result of 
      the phi function, depending on the runtime control flow. 
      my idea is simple, we can use the three 
      operand instruction and use the two source operands to specify the 
      range of incoming locals which are the incoming values. 
      Though, this does require that incoming 
      values be contiguous within the locals array, which may not be 
      possible or convienient. so we'll see.

  6 - "how do we allocate global constants and new types when BC IR
      is optimized towards working with local variables?"

      essentially, since we are going to represent expressions which 
      operate on local variables as stated in [5] how do we handle 
      global constants/variables? 
      (variables are semantically nearly identical to 
      constants, other than constness, so for the purposes of this discussion 
      their treatment is going to be identical.
      additionally new types are essentially global "constants" if types are 
      semantically similar to values, which theoretically speaking they are 
      in my opinion)

      global constants are interesting because they are a location where 
      expressions are allowed (binops and unops). but they are not held 
      within a function's scope. So they cannot be executed at runtime
      under the basic rules of evaluation. that is:

      OS invokes main() invokes ... returns-to main() returns-to OS

      in order for a global value to have semantic meaning within this 
      framework of evaluation, that global value (i.e. memory location) must 
      already be allocated, and be valid throughout the execution of the 
      program. Now, from the assembly perspective, this is accomplished by 
      allocating some space in the programs data or bss segments, and associating 
      there allocations with the corresponding labels, so this is understood.

      it is simple, they are evaluated at compile time. they are "comptime" values.
      However, compile time evaluation is essentially interpretation, so we must 
      add a phase to compilation, interpretation. 
      so, where does the bytecode which represents the global constant live?
      when we had a stack based, variable length instruction VM we had a
      "global function scope"/"script scope" where the bytecode itself had 
      instructions which defined global variables. local variables were simply 
      slots on the stack. That is defining a local variable was "lowered" to a 
      push operation.

      (I want to avoid adding opcodes as much as possible, 
      such that we stay under 255 opcodes total, for the full 
      feature rich language)

      I want to avoid adding an instruction just to define a global name. I would rather 
      the global names be stored such that the code simply knew what was a global and 
      how to define it.

      The idea that comes to mind is that functions store their own section of bytecode 
      to represent their body. (and perhaps that will be expanded to a list of sections
      of bytecode to represent control flow within the body of the function.)
      what if a global variable or constant or new type did the same?
      that is, when we parse the source code which defines a global, the initializer 
      expressions is represented as a new chunk of bytecode. 
      Defining that global becomes, interpret that bytecode down to a result, and then 
      initialize that global to the result. However, now we need bytecode which 
      can represent types, can represent other global variables, and can represent 
      other global constants.

      okay, for now, lets drop support for global constants/variables and new types.
      we are only going to support function definitions in this initial pass of the 
      compiler. Since we have to support function calls, this mechanism should 
      illuminate a potential pathway to referencing global names in bytecode.


7 - "how does the register allocator use the IR to allocate variables?"

    So, from my current research into this, the "linear scan register allocation"
    seems to work nicely. It is less complex than graph coloring, and apparently 
    generates code which approaches the quality produced by graph coloring 
    [https://dl.acm.org/doi/10.1145/330249.330250]. 
    There are several algorithms which improve upon the initial algortihm as well.
    (one that I am particularly interested in is: 
    [https://arxiv.org/abs/2011.05608])

    the pseudo-code for linear scan register allocation:

    LinearScanRegisterAllocation
    active ← {}
    for each live interval i, in order of increasing start point do
        ExpireOldIntervals(i)
        if length(active) = R then
            SpillAtInterval(i)
        else
            register[i] ← a register removed from pool of free registers
            add i to active, sorted by increasing end point

    ExpireOldIntervals(i)
        for each interval j in active, in order of increasing end point do
            if endpoint[j] ≥ startpoint[i] then
                return 
            remove j from active
            add register[j] to pool of free registers

    SpillAtInterval(i)
        spill ← last interval in active
        if endpoint[spill] > endpoint[i] then
            register[i] ← register[spill]
            location[spill] ← new stack location
            remove spill from active
            add i to active, sorted by increasing end point
        else
            location[i] ← new stack location

    The algorithm relies on a set of "liveness ranges" or "live intervals"
    which are precomputed for each basic block. The live interval is 
    simply the interval that a given value is "alive." That is the 
    range over which a variable is accessible within the current scope.

    In order to allocate registers, this algorithm needs an enumeration 
    of all available registers on the target machine. 
    We also need to pre-allocate registers for incoming function arguments,
    and for outgoing function arguments, and for certain instructions. 
    (division, bit rotate, bit shift, etc.)

    and this leads naturally into:

7a - "how does the backend work with the register allocator
    to generate target machine assembly?"

    There needs to be an "instruction selector" which can take the 
    information of what SSA locals are stored in what registers, 
    plus the information of what operations need to be performed 
    and select the correct assembly instruction. 
    Then, the selected assembly instructions can be written to the 
    output file.



(TODO: numbers are arbitrary and relative, they only exist to artifically 
       delineate thoughts.)
TODO: 
 -1 - we are removing most of the existing code in order to refactor it.
      as most of the existing code is not built up with the following 
      considerations in mind, and as such must be remade completely. 

  0 - globals are kept in a separate kind of symbol table, which has the ability to 
      map names to globally valid data. specifically named constants, named types,
      and functions.

  1 - we need a structure which can represent the local variables of a function 
      which can be accessed by instructions using an integer index; such that
      bytecode instructions can be encoded which reference these variables.

      the parser also has to handle the difference between a local and a global constant.



  3 - the static length instruction encoding needs to be decided. I am leaning towards 
      a u64 as the basis. so there is 64 bits or 8 bytes to work with per instruction.
      we have to have the opcode on any given instruction.
      if we say the opcode is a [u8], then we have 7 bytes left for operands,
      7 operands -> [u8][u8][u8][u8][u8][u8][u8]

      6 operands -> [u16][u8][u8][u8][u8][u8]

      5 operands -> [u16][u16][u8][u8][u8]

      4 operands -> [u16][u16][u16][u8]
                 -> [u32][u8][u8][u8]

      3 operands -> [u32][u16][u8]
                 -> [u24][u24][u8]
                 -> [u24][u16][u16]

      2 operands -> [u32][u24]
                 -> [u40][u16]
                 -> [u48][u8]

      1 operand  -> [u56]

      if we say the opcode is a [u16], then we have 6 bytes left for operands.
      some possible encodings:
      6 operands -> [u8][u8][u8][u8][u8][u8]

      5 operands -> [u16][u8][u8][u8][u8]

      4 operands -> [u16][u16][u8][u8]
                 -> [u24][u8][u8][u8]

      3 operands -> [u16][u16][u16]
                 -> [u32][u8][u8]

      2 operands -> [u24][u24]
                 -> [u32][u16]
                 -> [u40][u8]

      1 operand  -> [u48]

      now, the jump between a u8 and a u16 is significant, as we go from 
      2^8 -> 256 to 2^16 -> 65536.
      from my naieve perspective it seems like in SSA form 255 locals might 
      easily be reached by reasonable length functions, they would be long 
      yes, but perhaps not excessively so? this is intuition again however.
      whereas 65536 locals seems like you would have to have a pathalogically 
      long function in order to reach that many intermediate local variables. 
      (or maybe machine generated code)
      note that lua (the VM) has a hard limit of 200 locals for a function, due to 
      their 32 bit instruction encoding. but I dont think that lua bytecode is 
      in SSA form so that might not be a fair comparison.

      I think the instruction layouts I like from above are:
      u8  opcode -> [u8][u24][u16][u16]
                 -> [u8][u32][u24]
                 -> [u8][u56]

      u16 opcode -> [u16][u16][u16][u16]
                 -> [u16][u24][u24]
                 -> [u16][u48]

      the u16 opcode leaving us with 6 bytes left over allows for each 
      operand to be the same length, which I like, but this is nearly 
      completely arbitrary.

      the u8 opcode allows for all opcodes we could want (probably? I think?)
      but one operand is arbitrarily bigger than the others, which doesn't 
      matter all that much.

      this is also a consideration for the opcode itself, one thing I have noticed 
      is that when we are considering opcodes for a semantic act, such as add two numbers,
      we don't just need a form of the instruction for adding two immediate numbers, 
      but also a form for adding an immediate to a existing variable, and a form for adding 
      an two existing variables. And this observation holds for all of the other binary 
      operations that we want to encode. [+ - * / % & | ~ and or] which is 30 opcodes 
      just for "arithmetic" binary operators. this has me concerned that as we add more opcodes,
      such as unary operations [~ not], load/store, call/return, plus the opcodes 
      for working with structs/unions, jumps, conditions, and all their variants, 
      that 255 could also be reached rather easily, even under "normal"
      circumstances. This raises the concern that the number of operations may exceed 
      the number of available opcodes, but I am unsure how well founded that is.
      RISC-V's static 32 bit encoding only uses ~8 bits to encode "opcodes" 
      however RISC-V also has a fixed number of registers to encode, so there is more 
      flexibility there.
      Lua only has ~80 opcodes, and that is a full fledged register based VM.
      LLVM only has ~60 instructions, however the complexity of "variants" to standard 
      instructions like ADD, SUB, etc, is handled by polymorphism at a higher level 
      of abstraction than bytecode.

      so maybe 255 is plenty? 

      coming back to this after thinking more about the RISC-V encoding, how about 
      we "reserve" the second byte in the u16 opcode for another use.
      for instance, the first byte could store that the instruction is an add instruction,
      whereas the second byte could be used to say that the operands were immediates
      or registers, or something else. This both simplifies the opcodes themselves,
      because there doesn't need to be variant opcodes, and it allows for variant 
      opcode semantics to be expressed. (as a bonus the operands are equal lengths!)





